{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doJ0W9WU_oci",
        "outputId": "41886d57-ce1c-491e-f7bd-791571630dd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting rwkv\n",
            "  Downloading rwkv-0.8.26-py3-none-any.whl (406 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m406.2/406.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from rwkv) (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->rwkv) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->rwkv) (2024.2.2)\n",
            "Installing collected packages: rwkv\n",
            "Successfully installed rwkv-0.8.26\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "!pip install rwkv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfhWdAjbAVpq",
        "outputId": "5ad7b5d7-bdf5-4285-93c7-7e88def0ed15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI_GwRm5AgGR",
        "outputId": "70a3919b-fdc3-4c82-bf13-f5baa7b3f4a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-04-28 10:36:23--  https://huggingface.co/BlinkDL/temp-latest-training-models/resolve/main/RWKV-x060-chn_single_round_test-1B6-20240427-ctx1024.pth\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.114, 3.163.189.37, 3.163.189.74, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/20/96/209600910186b29a71a230a87a5a555fd33bc07c57dabd6f6f9fd03523c5326f/3d088400933a96187c2b19e8e04072a70b881ead563ecb8e6a4c866c88a0b811?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27RWKV-x060-chn_single_round_test-1B6-20240427-ctx1024.pth%3B+filename%3D%22RWKV-x060-chn_single_round_test-1B6-20240427-ctx1024.pth%22%3B&Expires=1714559783&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNDU1OTc4M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8yMC85Ni8yMDk2MDA5MTAxODZiMjlhNzFhMjMwYTg3YTVhNTU1ZmQzM2JjMDdjNTdkYWJkNmY2ZjlmZDAzNTIzYzUzMjZmLzNkMDg4NDAwOTMzYTk2MTg3YzJiMTllOGUwNDA3MmE3MGI4ODFlYWQ1NjNlY2I4ZTZhNGM4NjZjODhhMGI4MTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=baKb9IcfInh-G2fnYhnM1tidzpIMIkgf88sSx2ceLjX9gTI6rsUe71HdOhhh52%7Eq1bvtHsdNKzymukp85jK%7EveC98hKsRoCKZuqesFwf0%7EziNzTgRUDvmgxgXqySqjTEPm5rbVpT0gb3venwdOUuRb4Ms1m4s1ItXesp2aYoRAS2j7tH6-eWygB5TKO25gc2ceqFOMx9d5FLkTpju%7EgFU7V1jSJVQpCz2I%7Es3D%7EyXe0AGrbnR7rbajOGJcDpw8DDsvQluxMQCquRHB5qhY1rdYXzNP3DqojHYvS6kJKUWXdLjysLYoF1HHpdVyXf2BLkYM62AinDVi-HFnaRLCW%7EaQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-04-28 10:36:23--  https://cdn-lfs.huggingface.co/repos/20/96/209600910186b29a71a230a87a5a555fd33bc07c57dabd6f6f9fd03523c5326f/3d088400933a96187c2b19e8e04072a70b881ead563ecb8e6a4c866c88a0b811?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27RWKV-x060-chn_single_round_test-1B6-20240427-ctx1024.pth%3B+filename%3D%22RWKV-x060-chn_single_round_test-1B6-20240427-ctx1024.pth%22%3B&Expires=1714559783&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNDU1OTc4M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8yMC85Ni8yMDk2MDA5MTAxODZiMjlhNzFhMjMwYTg3YTVhNTU1ZmQzM2JjMDdjNTdkYWJkNmY2ZjlmZDAzNTIzYzUzMjZmLzNkMDg4NDAwOTMzYTk2MTg3YzJiMTllOGUwNDA3MmE3MGI4ODFlYWQ1NjNlY2I4ZTZhNGM4NjZjODhhMGI4MTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=baKb9IcfInh-G2fnYhnM1tidzpIMIkgf88sSx2ceLjX9gTI6rsUe71HdOhhh52%7Eq1bvtHsdNKzymukp85jK%7EveC98hKsRoCKZuqesFwf0%7EziNzTgRUDvmgxgXqySqjTEPm5rbVpT0gb3venwdOUuRb4Ms1m4s1ItXesp2aYoRAS2j7tH6-eWygB5TKO25gc2ceqFOMx9d5FLkTpju%7EgFU7V1jSJVQpCz2I%7Es3D%7EyXe0AGrbnR7rbajOGJcDpw8DDsvQluxMQCquRHB5qhY1rdYXzNP3DqojHYvS6kJKUWXdLjysLYoF1HHpdVyXf2BLkYM62AinDVi-HFnaRLCW%7EaQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.23, 108.138.94.14, 108.138.94.25, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3206274789 (3.0G) [binary/octet-stream]\n",
            "Saving to: ‘RWKV-x060-chn_single_round_test-1B6-20240427-ctx1024.pth’\n",
            "\n",
            "RWKV-x060-chn_singl 100%[===================>]   2.99G  60.0MB/s    in 67s     \n",
            "\n",
            "2024-04-28 10:37:30 (45.9 MB/s) - ‘RWKV-x060-chn_single_round_test-1B6-20240427-ctx1024.pth’ saved [3206274789/3206274789]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# !wget https://huggingface.co/BlinkDL/rwkv-6-world/resolve/main/RWKV-x060-World-3B-v2-20240228-ctx4096.pth\n",
        "!wget https://huggingface.co/BlinkDL/temp-latest-training-models/resolve/main/RWKV-x060-chn_single_round_test-1B6-20240427-ctx1024.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX05wloLDXas",
        "outputId": "bae894a9-b3ed-4ac3-8033-33e4ceb92a78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Installing collected packages: ninja, ftfy\n",
            "Successfully installed ftfy-6.2.0 ninja-1.11.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install ninja tqdm ftfy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZdwJCLKL_Wf",
        "outputId": "27a7111d-7a5c-4fd2-b26a-2953be3c1627"
      },
      "outputs": [],
      "source": [
        "########################################################################################################\n",
        "# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n",
        "########################################################################################################\n",
        "\n",
        "print(\"RWKV Chat Simple Demo\")\n",
        "\n",
        "import os, copy, types, gc, sys, re, json, csv\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "# from prompt_toolkit import prompt\n",
        "import torch\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "os.environ[\"RWKV_JIT_ON\"] = \"1\"\n",
        "os.environ[\"RWKV_CUDA_ON\"] = \"1\"  # '1' to compile CUDA kernel (10x faster), requires c++ compiler & cuda libraries\n",
        "\n",
        "from rwkv.model import RWKV\n",
        "from rwkv.utils import PIPELINE\n",
        "\n",
        "########################################################################################################\n",
        "\n",
        "args = types.SimpleNamespace()\n",
        "\n",
        "args.strategy = \"cuda fp16\"  # use CUDA, fp16\n",
        "\n",
        "args.MODEL_NAME = \"RWKV-x060-chn_single_round_test-1B6-20240427-ctx1024\"\n",
        "\n",
        "GEN_TEMP = 1.0\n",
        "GEN_TOP_P = 0.3\n",
        "GEN_alpha_presence = 0.0\n",
        "GEN_alpha_frequency = 1.0\n",
        "GEN_penalty_decay = 0.996\n",
        "\n",
        "CHUNK_LEN = 256  # split input into chunks to save VRAM (shorter -> slower, but saves VRAM)\n",
        "\n",
        "########################################################################################################\n",
        "\n",
        "print(f\"Loading model - {args.MODEL_NAME}\")\n",
        "model = RWKV(model=args.MODEL_NAME, strategy=args.strategy)\n",
        "pipeline = PIPELINE(model, \"rwkv_vocab_v20230424\")\n",
        "\n",
        "\n",
        "# def run_rnn(ctx, model_state=None):\n",
        "#     ctx = ctx.replace(\"\\r\\n\", \"\\n\")\n",
        "#     tokens = pipeline.encode(ctx)\n",
        "#     tokens = [int(x) for x in tokens]\n",
        "#     while len(tokens) > 0:\n",
        "#         out, model_state = model.forward(tokens[:CHUNK_LEN], model_state)\n",
        "#         tokens = tokens[CHUNK_LEN:]\n",
        "#     return out, model_state\n",
        "\n",
        "\n",
        "init_ctx = \"User: hi\" + \"\\n\\n\"\n",
        "init_ctx += \"Assistant: Hi. I am your assistant and I will provide expert full response in full details. Please feel free to ask any question and I will always answer it.\" + \"\\n\\n\"\n",
        "init_ctx += \"User:\"\n",
        "out0, INIT_STATE = model.forward(pipeline.encode(init_ctx), None)\n",
        "\n",
        "# INIT_STATE = None\n",
        "\n",
        "def response(msg, max_tokens=2000, temp=1.0, topp=0.3):\n",
        "    msg = msg.strip()\n",
        "    msg = re.sub(r\"\\n+\", \"\\n\", msg)\n",
        "\n",
        "    if len(msg) == 0:\n",
        "      raise ValueError(\"Please Say Something\")\n",
        "    model_state = None\n",
        "    out, model_state = model.forward(pipeline.encode(\" \" + msg + \"\\n\\nAssistant:\"), copy.deepcopy(INIT_STATE))\n",
        "    # print(\"\\nAssistant:\", end=\"\")\n",
        "    occurrence = {}\n",
        "    out_tokens = []\n",
        "    for i in range(max_tokens):\n",
        "        for n in occurrence:\n",
        "            out[n] -= GEN_alpha_presence + occurrence[n] * GEN_alpha_frequency # repetition penalty\n",
        "        out[0] -= 1e10  # disable END_OF_TEXT\n",
        "        token = pipeline.sample_logits(out, temperature=temp, top_p=topp)\n",
        "        tmp = pipeline.decode(out_tokens)\n",
        "        if \"\\n\\n\" in tmp:\n",
        "          tmp=tmp.strip().strip(\"\\n\")\n",
        "          print(tmp)\n",
        "          print()\n",
        "          return tmp\n",
        "        out, model_state = model.forward([token], model_state)\n",
        "        out_tokens += [token]\n",
        "\n",
        "        for xxx in occurrence:\n",
        "            occurrence[xxx] *= GEN_penalty_decay\n",
        "        occurrence[token] = 1 + (occurrence[token] if token in occurrence else 0)\n",
        "    tmp = pipeline.decode(out_tokens)\n",
        "    tmp=tmp.strip().strip(\"\\n\")\n",
        "    print(tmp)\n",
        "    print()\n",
        "    return tmp\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "questions = []\n",
        "\n",
        "with open(\"/content/drive/MyDrive/data_release.jsonl\", \"r\", encoding=\"UTF8\") as fp:\n",
        "    for line in fp.readlines():\n",
        "        data = json.loads(line)\n",
        "        questions.append((data[\"question\"], data[\"category\"]) )\n",
        "j = 0\n",
        "\n",
        "with open('/content/drive/MyDrive/example_rwkv6_1b6_chn_single.csv', 'w', encoding='utf-8-sig') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for (q, r) in questions:\n",
        "        j += 1\n",
        "        print(j)\n",
        "        q = q.strip()\n",
        "        a = response(q, topp=(0.1 if r in [\"逻辑推理\",\"数学计算\",\"专业能力\"] else 0.7))\n",
        "        writer.writerow([a])\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
